:PROPERTIES:
:ID:       63ea08ba-bc6d-49f9-9193-ad3a18bb1d15
:END:
#+title: FFN

* 结构
FFN 的结构如下：

\( FFN(x) = W_2 · Activation(W_1·x + b_1) + b_2 \)

其中中间维度通常比输入维度更大（例如，在原始 [[id:3f59ec45-2231-4567-ba48-fd28fbf9db7a][Transformer]] 中，输入维度是 512，中间维度是 2048）。扩展维度的作用是让模型在更高维空间中对特征进行组合，形成更复杂的表示。

* FFN 与 MLP
FFN（前馈神经网络）和MLP（多层感知机）实际上是同一个概念的不同名称。两者都指代一种由多层线性变换和非线性激活函数组成的神经网络结构。

FFN 强调的是信息流是单向的，从输入层经过隐藏层到输出层，没有循环或反馈，与之形成对比的是 [[id:3cf528c9-514a-4d17-92db-7261eacc7410][RNN]] 。而 MLP 更偏向于一种具体的实现。

* 在 LLM 中
由于 FFN 中间维度的扩展，它在 Transformer 模型中占据了大部分参数（例如在原始论文中，FFN 参数约占每层总参数的 2/3）。这表明 FFN 是模型的核心计算模块之一，直接影响了模型容量。这个现象在 [[id:7072aa4a-2c58-4102-8c82-b6f9fb6fdcb9][MoE]] 中会变得更加明显。

Attention 机制并没有进行过多的现象变换，它更像是让 embedding 向量之间有一个受到上下文影响的重新分配。而 FFN 才是真正 *拓展、挖掘、拟合语义* 的过程。
